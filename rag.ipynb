{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.loader import load_docs\n",
    "from app.splitter import TextSplitter, token_size\n",
    "from nomic import embed\n",
    "from app.vector_store import VectorStore\n",
    "from pdfminer.high_level import extract_text\n",
    "from app.config import settings\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:45<00:00,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 PDF documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = load_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: 45366 tokens, 91 chunks\n",
      "Doc 2: 65671 tokens, 131 chunks\n",
      "Doc 3: 25519 tokens, 51 chunks\n",
      "Doc 4: 46689 tokens, 94 chunks\n",
      "Doc 5: 48980 tokens, 98 chunks\n",
      "Doc 6: 61540 tokens, 126 chunks\n",
      "Doc 7: 24178 tokens, 49 chunks\n",
      "Doc 8: 50730 tokens, 102 chunks\n",
      "Doc 9: 41999 tokens, 84 chunks\n",
      "Doc 10: 46333 tokens, 93 chunks\n",
      "Doc 11: 50055 tokens, 101 chunks\n",
      "\n",
      "Total chunks 1020\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "text_splitter = TextSplitter(chunk_size=512)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_chunks = text_splitter(doc)\n",
    "    chunks += doc_chunks\n",
    "    print(f'Doc {i+1}: {token_size(doc)} tokens, {len(doc_chunks)} chunks')\n",
    "\n",
    "print('\\nTotal chunks', len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 46333 tokens, 93 chunks\n"
     ]
    }
   ],
   "source": [
    "doc = extract_text('data/docs/Inception.pdf')\n",
    "text_splitter = TextSplitter(chunk_size=512)\n",
    "chunks = text_splitter(doc)\n",
    "print(f'Doc: {token_size(doc)} tokens, {len(chunks)} chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 93, Usage: {'prompt_tokens': 39334, 'total_tokens': 39334}\n"
     ]
    }
   ],
   "source": [
    "output = embed.text(\n",
    "    texts=chunks,\n",
    "    model='nomic-embed-text-v1.5',\n",
    "    task_type='search_document'\n",
    ")\n",
    "print(f'Embeddings: {len(output['embeddings'])}, Usage: {output['usage']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 vectors added to vector store\n"
     ]
    }
   ],
   "source": [
    "vector_store = VectorStore()\n",
    "vectors = [{'vector': vector, 'text': text} for vector, text in zip(output['embeddings'], chunks)]\n",
    "vector_store.add(vectors)\n",
    "print(f'{len(vectors)} vectors added to vector store')\n",
    "vector_store.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "vector_store.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's the most resilient parasite?\"\n",
    "query_embed = embed.text(\n",
    "    texts=[query],\n",
    "    model='nomic-embed-text-v1.5',\n",
    "    task_type='search_query'\n",
    ")\n",
    "query_vector = query_embed['embeddings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.query(query_vector)\n",
    "for res in results:\n",
    "    print(res['score'])\n",
    "    print(res['text'])\n",
    "    print('*'*50)\n",
    "    print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = Groq(api_key=settings.GROQ_API_KEY)\n",
    "\n",
    "vector_store = VectorStore()\n",
    "vector_store.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an assistant that answers user questions about a collection of movie screenplays.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Use the following pieces of context from movie screenplays to answer the user question.\n",
    "You must only use the facts from the context to answer. If the answer cannot be found in the context, say that you don't have enough information to answer the question and provide any facts from the context that could be relevant to the answer.\n",
    "Don't address the context or the scripts directly in your answer, just answer the question like it's your own knowledge.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    query_embed = embed.text(\n",
    "        texts=[question],\n",
    "        model='nomic-embed-text-v1.5',\n",
    "        task_type='search_query'\n",
    "    )\n",
    "    query_vector = query_embed['embeddings'][0]\n",
    "    chunks = vector_store.query(query_vector)\n",
    "\n",
    "    context = '\\n\\n---\\n\\n'.join([chunk['text'] for chunk in chunks]) + '\\n\\n---'\n",
    "    user_message =  USER_PROMPT.format(context=context, question=question)\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "\n",
    "    chat_completion = groq_client.chat.completions.create(\n",
    "        messages=messages, model='llama3-70b-8192'\n",
    "    )\n",
    "    print('Total tokens:', chat_completion.usage.total_tokens)\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 5172\n",
      "What's the most resilient parasite?\n",
      "According to Cobb, the most resilient parasite is an idea. He explains that once an idea takes hold in someone's brain, it's almost impossible to eradicate, even if the person tries to cover it up or ignore it. \n",
      "\n",
      "Total tokens: 5167\n",
      "What's the name of Cobb's wife\n",
      "The name of Cobb's wife is Mal. \n",
      "\n",
      "Total tokens: 5200\n",
      "What's the name of Fisher's company\n",
      "The name of Robert Fischer's company is Fischer Morrow. It's an energy conglomerate. \n",
      "\n",
      "Total tokens: 5188\n",
      "How did Cobb die?\n",
      "I don't have enough information to answer this question. The provided context does not explicitly state how Cobb died. In fact, there is no indication that Cobb died at all. \n",
      "\n",
      "Total tokens: 5320\n",
      "what was Fisher's relationship with his father like?\n",
      "According to the script, Fischer's relationship with his father, Maurice Fischer, was strained and difficult. In a conversation with Browning, Fischer says that after his mother died, he went to his father in his grief, but his father told him, \"There's really nothing to be said, Robert.\" Fischer also reveals that his father, on his deathbed, said only one word to him: \"Disappointed.\" This suggests that Maurice Fischer was emotionally distant and critical towards his son. \n",
      "\n",
      "Total tokens: 5166\n",
      "What happened to Cobb's wife?\n",
      "Mal, Cobb's wife, died. The exact circumstances of her death are not clearly stated in the provided context, but it is implied that she jumped to her death from a great height in a hotel room, and that Cobb was with her at the time. Cobb blames himself for her death and feels guilty about it. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What's the most resilient parasite?\",\n",
    "    \"What's the name of Cobb's wife\",\n",
    "    \"What's the name of Fisher's company\",\n",
    "    \"what was Fisher's relationship with his father like?\",\n",
    "    \"What happened to Cobb's wife?\",\n",
    "    \"How did Cobb die?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = answer_question(question)\n",
    "    print(question)\n",
    "    print(answer, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
